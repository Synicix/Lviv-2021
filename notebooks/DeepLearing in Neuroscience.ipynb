{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
    "<a target=\"_blank\"  href=\"https://colab.research.google.com/github/eywalker/LVIV-2021/blob/main/notebooks/DeepLearing%20in%20Neuroscience.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
    "</td><td>\n",
    "<a target=\"_blank\"  href=\"https://github.com/eywalker/LVIV-2021/blob/main/notebooks/DeepLearing%20in%20Neuroscience.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Welcome to Deep Learning in Neuroscience by Edgar Y. Walker"
   ],
   "metadata": {
    "id": "xp0SO4gr2D7w"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a Jupyter notebook to accompany the course on \"Deep Learning in Neuroscience\" taught as part of the Lviv Data Science Summer School 2021. This notebook as well as any other relevant information can be found in the [GitHub repository](https://github.com/eywalker/lviv-2021)!\n",
    "\n",
    "In this course, we will learn how deep learning is getting utilized in studying neuroscience, specifically in building models of neurons to complex sensory inputs such as natural images. We will start by going through some neuroscience primer. We will then get our hands dirty by taking real neuronal responses recorded from mouse primary visual cortex (V1) as the mouse observes a bunch of natural images and developing a model to predict these responses. By the end of this course, you will gain some basic familiarity in utilizing deep learning models to predict responses of 1000s of neurons to natural images!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### <font color='red'>NOTE: Please run this section at the very beginning of the first session!</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we get to dive in and learn how deep learning is used in neuroscience and get your first neural predictive model trained, we need to install some prerequisite packages and download some neuronal data!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting the code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to primarily use [PyTorch](https://pytorch.org) to build, train and evaluate our deep learning models and I am going to assume some familiarity with PyTorch already.\n",
    "\n",
    "Also to be able to handle the dataset containing neuronal activities, we are going to make our life easier by using a few existing libraries. I have prepared a library called [lviv2021](https://github.com/eywalker/lviv2021). This library has a dependency on [neuralpredictors](https://github.com/sinzlab/neuralpredictors), which is a collection of PyTorch layers, tools and other utilities that would prove helpful to train networks to predict neuronal responses.\n",
    "\n",
    "Let's go ahead and install this inside the Colab environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Install PyTorch dependency\n",
    "!pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    \n",
    "# Install \n",
    "!pip3 install git+https://github.com/eywalker/lviv-2021.git"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "m7dJ_i-tSPEq",
    "outputId": "bdc3f3f8-e451-4bd3-83d5-3f126b88f431"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to use the dataset made available for our recent paper [Lurz et al. ICLR 2021](https://github.com/sinzlab/Lurz_2020_code), predicting responses of mouse visual cortex to natural images. \n",
    "\n",
    "The dataset can take anywhere from 5-10 min to download, so please be sure to **run the following at the very beginning of the session!** We are going to first spend some time learning the basics of computational neuroscience in the study of system identification. It would be best that you let the download take place while we go over the neursocience primer so that it will be ready when we come back here to get our hands dirty!\n",
    "\n",
    "To download the data, simply execute the following cell, and let it run till completion."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!git clone https://gin.g-node.org/cajal/Lurz2020.git /content/data"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsNehEZlTSL4",
    "outputId": "8e2984ad-9f32-4dfd-b8b7-4536f2e53823"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Developing models of neural population responses to natural images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that you have been primed with just enough background neuroscience, let's get our hand dirty and try to build our first neural predictive models.\n",
    "\n",
    "As part of the setup, we have downloaded a 2-photon imaging dataset from mouse primary visual cortex as we present 1000s of natural images (if not done yet, please do so immediately by stepping through the beginning sections of this notebook)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Navigating the neuroscience data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As with any data science project, you must start by understanding your data! Let's take some time to navigate the data you downloaded."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "ls ./data/static20457-5-9-preproc0/"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "change.log  config.json  \u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mmeta\u001b[0m/\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ls ./data/static20457-5-9-preproc0/data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ls ./data/static20457-5-9-preproc0/data/responses | head -30"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ls ./data/static20457-5-9-preproc0/data/images | head -30"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see that both responses and contained in collections of `numpy` files named like `1.npy` or `31.npy`. The number here corresponds to a specific **trial** or simply different image presentation during an experiment.\n",
    "\n",
    "Let's take a look at some of these files."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading data files one at a time"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's pick some trial and load the image as well as the response"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trial_idx = 1100\n",
    "trial_image = np.load(f'./data/static20457-5-9-preproc0/data/images/{trial_idx}.npy')\n",
    "trial_responses = np.load(f'./data/static20457-5-9-preproc0/data/responses/{trial_idx}.npy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The image is shaped as $\\text{channel} \\times \\text{height} \\times \\text{width}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trial_image.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.imshow(trial_image.squeeze(), cmap='gray', vmin=0, vmax=255)\n",
    "plt.axis('off')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In contrast, the shape of `trial_response` is simply the number of neurons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trial_responses.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trial_responses.min() # responses are practically always >= 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trial_responses.max()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(1, 1, dpi=150)\n",
    "ax.hist(trial_responses, 100);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see most neuron's responses stay very close to 0 - signifying no activity."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the entire dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "While we can inspect the image and the corresponding neural population responses one image at a time, this is quite cumbersome and also impractical for use in network training. Fortunately, the `lviv` package provides us with a convenience function that will help to load the entire dataset as PyTorch dataloaders."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from lviv.dataset import load_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we prepare the dataloaders, we get to specify the batch size."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "dataloaders = load_dataset(path = '/content/data/static20457-5-9-preproc0', batch_size=60)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function returns a dictionary consisting of three dataloaders for training, validation, and test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataloaders"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's specifically look at the trainset dataloader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "train_loader = dataloaders['train']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Total number of images can be checked as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "len(train_loader.sampler)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4472"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can inspect what it returns per batch:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "images, responses = next(iter(train_loader))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "images.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([60, 1, 36, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "responses.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([60, 5335])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected, you can see it returns a batch size of 60 images and responses for all neurons."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar inspection can be done on the **validation** and **testing** dataloaders."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# number of images in validation set\n",
    "len(dataloaders['validation'].sampler)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "522"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# number of images in test set\n",
    "len(dataloaders['test'].sampler)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You might think that we have a lot of images in test set, but this is because test set consists of repeated images."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some additional trial information can be observed by accessing the underlying PyTorch dataset object and looking at the `trial_info`. Note that this is not part of the standard PyTorch dataset/dataloader interface, but rather a feature specifically provided by the library!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Access to the dataset object that underlies all dataloaders\n",
    "testset = dataloaders['test'].dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "test_trials = np.where(testset.trial_info['tiers'] == 'test')[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "image_ids = testset.trial_info['frame_image_id']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "np.unique(image_ids[test_trials])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 104,  128,  183,  355,  479,  483,  656,  803,  830,  936, 1201,\n",
       "       1494, 1596, 1652, 1656, 1682, 1731, 1756, 1796, 2005, 2008, 2014,\n",
       "       2159, 2214, 2389, 2586, 2710, 2746, 2747, 2803, 2816, 2825, 2954,\n",
       "       3018, 3107, 3144, 3163, 3372, 3427, 3438, 3487, 3507, 3562, 3702,\n",
       "       3847, 3924, 4231, 4295, 4373, 4397, 4400, 4430, 4594, 4619, 4667,\n",
       "       4674, 4717, 4739, 4782, 4812, 4814, 4821, 4923, 4953, 5034, 5128,\n",
       "       5166, 5225, 5264, 5288, 5322, 5334, 5399, 5402, 5504, 5640, 5671,\n",
       "       5679, 5754, 5782, 6013, 6034, 6066, 6082, 6205, 6238, 6248, 6490,\n",
       "       6562, 6773, 6790, 6831, 6886, 7017, 7028, 7107, 7119, 7120, 7154,\n",
       "       7495])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "len(np.unique(image_ids[test_trials]))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So you can see that the test set consists of 100 unique images, each repeated up to 10 times."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "testset.trial_info.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['trial_idx',\n",
       " 'session',\n",
       " 'frame_trial_ts',\n",
       " 'frame_last_flip',\n",
       " 'frame_image_id',\n",
       " 'frame_image_class',\n",
       " 'frame_pre_blank_period',\n",
       " 'condition_hash',\n",
       " 'tiers',\n",
       " 'animal_id',\n",
       " 'scan_idx',\n",
       " 'frame_presentation_time']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "testset.trial_info.frame_image_id  # gives information about presented image ID"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1301, 5927, 3982, ...,  464,  819, 3025])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "testset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FileTreeDataset /content/data/static20457-5-9-preproc0 (n=5993 items)\n",
       "\timages, responses"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "dataloaders"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7fbcf66025b0>,\n",
       " 'validation': <torch.utils.data.dataloader.DataLoader at 0x7fbcf66024f0>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7fbcf66021f0>}"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "len(dataloaders['validation'].dataset.trial_info.frame_image_id)  # gives information about presented image ID"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5993"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling the neuronal responses"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have successfully loaded the dataset and inspected its contents, it's time for us to start **modeling** the responses.\n",
    "\n",
    "We will start by building a very basic **Linear-Nonlinear model** - which is nothing more than a shallow neural network with single linear layer followed by an activation function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear-Nonlinear (LN) model"
   ],
   "metadata": {
    "id": "TcVMbaG9xXNY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Background"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Arguably one of the simplest model of a neuron's response to a stimulus is the **linear-nonlinear (LN) model**. \n",
    "\n",
    "Given an image $I \\in \\mathbb{R}^{h\\,\\times\\,w}$ where $h$ and $w$ are the height and the width of the image, respectively, let us collapse the image into a vector $x \\in \\mathbb{R}^{hw}$.\n",
    "\n",
    "A single neuron's response $r$ under linear-nonlinear model can then be expressed as:\n",
    "$$\n",
    "r = a(w^\\top x + b),\n",
    "$$\n",
    "where $w \\in \\mathbb{R}^{hw}$ and $b \\in \\mathbb{R}$ are **weight** and **bias**, and $a:\\mathbb{R}\\mapsto\\mathbb{R}$ is a scalar **activating function**.\n",
    "\n",
    "We can in fact extend to capture the responses of all $N$ neurons simultaneously as:\n",
    "\n",
    "$$\n",
    "\\mathbf{r} = a(\\mathbf{W} x + \\mathbf{b}),\n",
    "$$\n",
    "where $\\mathbf{r} \\in \\mathbb{R}^{N}$, $\\mathbf{W} \\in \\mathbb{R}^{N\\,\\times\\,hw}$ and $\\mathbf{b} \\in \\mathbb{R}^{N}$.\n",
    "\n",
    "Hence, each neuron weights each pixel of the image according to the weight $w$ (a column of $\\mathbf{W}$) and thus characterizes how much the each neuron \"cares\" about a specific pixel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The nonlinear activation function $a(\\cdot)$ ensures, among other things, that the output of the network stays above 0. In fitting neuronal responses, we tend to use $a(x) = ELU(x) + 1$ where ELU (Exponential Linear Unit) is defined as follows:\n",
    "\n",
    "$$\n",
    "    ELU(x) = \n",
    "\\begin{cases}\n",
    "    e^x - 1, & x \\lt 0 \\\\\n",
    "    x,   & x \\ge 0\n",
    "\\end{cases}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plotting ELU function\n",
    "x = np.linspace(-2, 2)\n",
    "plt.plot(x, F.elu(torch.Tensor(x)))\n",
    "plt.axhline(0, c='r', ls='--')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We shift it by 1 to ensure it will always remain positive"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plotting ELU+1 function\n",
    "x = np.linspace(-2, 2)\n",
    "plt.plot(x, F.elu(torch.Tensor(x))+1)\n",
    "plt.axhline(0, c='r', ls='--')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Overall, it can be seen that a linear-nonlinear is nothing more than a single linear layer on flattened image, followed by a nonlinear activation. Now let's go ahead and implment our LN model in PyTorch!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We therefore go ahead and implement a simple network consisting of a linear layer followed by ELU + 1 activation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_height,\n",
    "        input_width,\n",
    "        n_neurons,\n",
    "        momentum=0.1,\n",
    "        init_std=1e-3,\n",
    "        gamma=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(1, momentum=momentum, affine=False)\n",
    "        self.linear = nn.Linear(input_height * input_width, n_neurons)\n",
    "        self.gamma = gamma\n",
    "        self.init_std = init_std\n",
    "        self.initialize()\n",
    "        \n",
    "\n",
    "    def initialize(self, std=None):\n",
    "        if std is None:\n",
    "            std = self.init_std\n",
    "        nn.init.normal_(self.linear.weight.data, std=std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.linear(x.flatten(1))\n",
    "        return nn.functional.elu(x) + 1\n",
    "\n",
    "    def regularizer(self):\n",
    "        return self.gamma * self.linear.weight.abs().sum()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And that's it! We have now designed our first network model of the neuron's responses!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**BONUS**: notice that we used batch normalization (BN) layer right before the linear layer? This empirically helps to stabilize the training, allowing us to be not too sensitive to the weight and bias initialization. You could totally implement and train a LN network without such BN layer and you are more than welcome to try! However if you do, be very aware of the network weight initializations and the chocie of learning rate during the training."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, let's instantiate the model before we move onto the next step of training the model!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "ln_model = Linear(input_height=64, input_width=36, n_neurons=5335, gamma=0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have a candidate model designed, it's time to train it. While we could use standard set of optimizers as provided by PyTorch to implement our training routine, here we are provided with a convenience function `train_model` that would handle a lot of the training boiler plate."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from lviv.trainers import train_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Critically, `train_model` sets up training based on **Poisson loss** and also perform early stopping based on **correlation** of the predicted neuronal responses with the actual neuronal responses on the **validation set**. Let's now talk briefly about our objective (loss) function of choice in training neuron response models - the Poisson loss."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mathematical aside: Poisson Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### How we are **actually** modeling the noisy neuronal responses"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The use of **Poisson loss** follows from the assumption that, *conditioned on the stimulus*, the neurons' responses follow an **independent Poisson** distribution. That is, given an input image $x$, the population response $\\mathbf{r}$ is distributed as:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{r} | x) = \\prod_i^N \\text{Poiss}(r_i; \\lambda_i(x))\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "where $r_i$ is the $i^\\text{th}$ neuron in the population $\\mathbf{r}$. The $\\lambda_i$ is the parameter for Poisson distribution that controls its **average value**. Here we express $\\lambda_i(x)$ to indicate the fact that the average response for each neuron is expected to vary *as a function of the input image*. We can express this average matching as:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[r_i|x] = \\lambda_i(x)\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In fact, it is precise this function $\\lambda_i(x)$ that we are modeling using LN models and, in the next step, more complex neural networks. In otherwords, we are learning $\\lambda_i(x) = f_i(x, \\theta)$, where $\\theta$ is the trainable parameters of the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Putting all together, this means that, our model $f(x, \\theta)$ is really modeling the average activity of the neurons,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mathbf{r}|x] = \\mathbf{f}(x, \\theta)\n",
    "$$\n",
    "\n",
    "while we are assuming that the neurons are distribution according to **independent Poisson** distribution around the average responses by our model $\\mathbf{f}(x, \\theta)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Deriving the objective function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Poisson distribution is defined as follows:\n",
    "\n",
    "$$\n",
    "p(r) = \\text{Poiss}(r; \\lambda) = \\frac{e^{-\\lambda}\\lambda^{r}}{r!}\n",
    "$$\n",
    "\n",
    "During the training, we would want to adjust the model parameter $\\theta$ to maximize the chance of observing the response $\\mathbf{r}$ to a known image $x$. This is achieved by **maximizing** the log-likelihood function $\\log p(\\mathbf{r}|x, \\theta)$, or equivalently by **minimzing the negative log-likelihood function** as the objective function $L(x, \\mathbf{r}, \\theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(x, \\mathbf{r}, \\theta) &= -\\log p(\\mathbf{r}|x, \\theta) \\\\\n",
    "&= -\\log \\prod_i \\text{Poiss}(r_i; f_i(x, \\theta)) \\\\\n",
    "&= -\\sum_i \\log \\frac{e^{-f_i(x, \\theta)}f_i(x, \\theta)^{r_i}}{r_i!} \\\\\n",
    "&= \\sum_i \\left(f_i(x, \\theta) - r_i \\log f_i(x, \\theta) + \\log r_i! \\right)\n",
    "\\end{align}\n",
    "$$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "During the optimization, we seek for $\\theta$ that would minimize the loss $L$. Note that since the term $log r_i!$ does not depend on $\\theta$, it can be safely dropped from Poisson loss. Hence you would commonly see the following expression as the definition of the **Poisson loss**\n",
    "\n",
    "$$\n",
    "L_\\text{Poiss}(x, \\mathbf{r}, \\theta) = \\sum_i \\left(f_i(x, \\theta) - r_i \\log f_i(x, \\theta)\\right)\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Performing the training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the theoretical foundation for the training and the choice of the objective function under our belt, let's go ahead and train the network. Because the function `train_model` handles a lot underneath the hood, training a model is just as easy as invoking the function by passing it the model to be trained and the dataloaders!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from lviv.trainers import train_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "score, output, model_state = train_model(model=ln_model, dataloader=dataloaders)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=======================================\n",
      "correlation -0.0019075753\n",
      "poisson_loss 9395851.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1: 100%|██████████| 150/150 [00:03<00:00, 44.69it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[001|00/05] ---> 0.05955984443426132\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=======================================\n",
      "correlation 0.059559844\n",
      "poisson_loss 3384139.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 2: 100%|██████████| 150/150 [00:00<00:00, 176.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[002|00/05] ---> 0.06592471897602081\n",
      "=======================================\n",
      "correlation 0.06592472\n",
      "poisson_loss 3274338.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 3: 100%|██████████| 150/150 [00:00<00:00, 177.13it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[003|01/05] -/-> 0.06381010264158249\n",
      "=======================================\n",
      "correlation 0.0638101\n",
      "poisson_loss 3274534.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 4: 100%|██████████| 150/150 [00:00<00:00, 174.58it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[004|01/05] ---> 0.07152469456195831\n",
      "=======================================\n",
      "correlation 0.071524695\n",
      "poisson_loss 3243809.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 5: 100%|██████████| 150/150 [00:00<00:00, 175.18it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[005|00/05] ---> 0.07718321681022644\n",
      "=======================================\n",
      "correlation 0.07718322\n",
      "poisson_loss 3174631.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 6: 100%|██████████| 150/150 [00:00<00:00, 176.82it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[006|00/05] ---> 0.07769027352333069\n",
      "=======================================\n",
      "correlation 0.07769027\n",
      "poisson_loss 3153348.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 7: 100%|██████████| 150/150 [00:00<00:00, 176.00it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[007|01/05] -/-> 0.07006139308214188\n",
      "=======================================\n",
      "correlation 0.07006139\n",
      "poisson_loss 3368900.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 8: 100%|██████████| 150/150 [00:00<00:00, 176.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[008|02/05] -/-> 0.06875057518482208\n",
      "=======================================\n",
      "correlation 0.068750575\n",
      "poisson_loss 3328448.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 9: 100%|██████████| 150/150 [00:00<00:00, 177.04it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[009|03/05] -/-> 0.06966907531023026\n",
      "=======================================\n",
      "correlation 0.069669075\n",
      "poisson_loss 3304112.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 10: 100%|██████████| 150/150 [00:00<00:00, 176.75it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[010|04/05] -/-> 0.0717167928814888\n",
      "=======================================\n",
      "correlation 0.07171679\n",
      "poisson_loss 3497721.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 11: 100%|██████████| 150/150 [00:00<00:00, 173.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[011|05/05] -/-> 0.07496411353349686\n",
      "Restoring best model after lr decay! 0.074964 ---> 0.077690\n",
      "=======================================\n",
      "correlation 0.07769027\n",
      "poisson_loss 3153348.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 12: 100%|██████████| 150/150 [00:00<00:00, 173.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    12: reducing learning rate of group 0 to 1.5000e-03.\n",
      "[012|01/05] -/-> 0.07143863290548325\n",
      "=======================================\n",
      "correlation 0.07143863\n",
      "poisson_loss 3309589.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 13: 100%|██████████| 150/150 [00:00<00:00, 174.99it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[013|01/05] ---> 0.10395169258117676\n",
      "=======================================\n",
      "correlation 0.10395169\n",
      "poisson_loss 2500118.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 14: 100%|██████████| 150/150 [00:00<00:00, 175.65it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[014|01/05] -/-> 0.10067544877529144\n",
      "=======================================\n",
      "correlation 0.10067545\n",
      "poisson_loss 2479766.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 15: 100%|██████████| 150/150 [00:00<00:00, 177.24it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[015|02/05] -/-> 0.10142096132040024\n",
      "=======================================\n",
      "correlation 0.10142096\n",
      "poisson_loss 2414144.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 16: 100%|██████████| 150/150 [00:00<00:00, 176.30it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[016|03/05] -/-> 0.10160883516073227\n",
      "=======================================\n",
      "correlation 0.101608835\n",
      "poisson_loss 2430222.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 17: 100%|██████████| 150/150 [00:00<00:00, 176.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[017|04/05] -/-> 0.09739978611469269\n",
      "=======================================\n",
      "correlation 0.097399786\n",
      "poisson_loss 2449914.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 18: 100%|██████████| 150/150 [00:00<00:00, 170.65it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[018|05/05] -/-> 0.09415452182292938\n",
      "Restoring best model after lr decay! 0.094155 ---> 0.103952\n",
      "=======================================\n",
      "correlation 0.10395169\n",
      "poisson_loss 2500118.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 19: 100%|██████████| 150/150 [00:00<00:00, 176.22it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    19: reducing learning rate of group 0 to 4.5000e-04.\n",
      "[019|01/05] -/-> 0.09806245565414429\n",
      "=======================================\n",
      "correlation 0.098062456\n",
      "poisson_loss 2533733.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 20: 100%|██████████| 150/150 [00:00<00:00, 174.56it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[020|01/05] ---> 0.11151435226202011\n",
      "=======================================\n",
      "correlation 0.11151435\n",
      "poisson_loss 2310042.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 21: 100%|██████████| 150/150 [00:00<00:00, 175.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[021|01/05] -/-> 0.10864903032779694\n",
      "=======================================\n",
      "correlation 0.10864903\n",
      "poisson_loss 2329606.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 22: 100%|██████████| 150/150 [00:00<00:00, 176.26it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[022|02/05] -/-> 0.10798943787813187\n",
      "=======================================\n",
      "correlation 0.10798944\n",
      "poisson_loss 2349926.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 23: 100%|██████████| 150/150 [00:00<00:00, 176.19it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[023|03/05] -/-> 0.10652267187833786\n",
      "=======================================\n",
      "correlation 0.10652267\n",
      "poisson_loss 2274603.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 24: 100%|██████████| 150/150 [00:00<00:00, 175.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[024|04/05] -/-> 0.10494276136159897\n",
      "=======================================\n",
      "correlation 0.10494276\n",
      "poisson_loss 2338915.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 25: 100%|██████████| 150/150 [00:00<00:00, 177.24it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[025|05/05] -/-> 0.1067856028676033\n",
      "Restoring best model after lr decay! 0.106786 ---> 0.111514\n",
      "Restoring best model! 0.111514 ---> 0.111514\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnIjtCeERCjk",
    "outputId": "569f4934-9cb5-4253-ae02-001b6a9d7828"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyzing the trained network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Woohoo! We have now successfully trained our very first LN model on real neuronal responses! But really, how good is the model?\n",
    "\n",
    "During the training, the `train_model` function iteratively reported two values: the loss function (Poisson loss) value and the average correlation. \n",
    "\n",
    "But what is this correlation? It's simply the correlation computed between our predicted neuronal responses $\\hat{r}_i$ and the actual neuronal responses $r_i$ across images in the validation set. We then take the average correlation value **across neurons** to get average correlation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Being a correlation, the highest possible value is of course 1.0, but practically this is never reached both due to 1) imperfection of our model but more fundamentally due to the noiseness of the neuron's responses. Because of the noise, even a perfect model would never reach a correlation of 1.0."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color='red'>\n",
    "    NOTE TO SELF: Add more here probably plotting some scatter plot for an example neuron, histogram of correlation scores both done on the testset.\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Going beyond Linear-Nonlinear model by using CNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We saw that a simple LN model can be trained to achieve above chance performance in predicting the responses of mouse V1 neurons to natural images. But we certainly must be able to do better than that, right?\n",
    "\n",
    "In the past decase, what has really driven system identification in visual neurons has been the use of convolutional neural networks (CNN). Below, we will try out a very simple CNN to see if we can already reach better performance than LN."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color='green'>\n",
    "    NOTE to collaborators: \n",
    "    Please add a simpler implementation of CNN. Ideally it would train just as fast as the simple fully connected linear model given above. \n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from collections import OrderedDict\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_height,\n",
    "        input_width,\n",
    "        n_neurons,\n",
    "        momentum=0.1,\n",
    "        init_std=1e-3,\n",
    "        gamma=0.1,\n",
    "        hidden_channels=8,\n",
    "    ):\n",
    "        super(CNN, self).__init__()\n",
    "        self.init_std = init_std\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # CNN core\n",
    "        self.cnn_core = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"conv1\", nn.Conv2d(1, hidden_channels, 15, padding=15 // 2, bias=False)),\n",
    "                    (\"bn1\", nn.BatchNorm2d(hidden_channels, momentum=momentum)),\n",
    "                    (\"elu1\", nn.ELU()),\n",
    "                    (\"conv2\", nn.Conv2d(hidden_channels, hidden_channels, 13, padding=13 // 2, bias=False)),\n",
    "                    (\"bn2\", nn.BatchNorm2d(hidden_channels, momentum=momentum)),\n",
    "                    (\"elu2\", nn.ELU()),\n",
    "                    (\"conv3\", nn.Conv2d(hidden_channels, hidden_channels, 13, padding=13 // 2, bias=False)),\n",
    "                    (\"bn3\", nn.BatchNorm2d(hidden_channels, momentum=momentum)),\n",
    "                    (\"elu3\", nn.ELU()),\n",
    "                    (\"conv4\", nn.Conv2d(hidden_channels, hidden_channels, 13, padding=13 // 2, bias=False)),\n",
    "                    (\"bn4\", nn.BatchNorm2d(hidden_channels, momentum=momentum)),\n",
    "                    (\"elu4\", nn.ELU()),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Fully connected readout\n",
    "        self.readout = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    ('fc_ro', nn.Linear(input_height * input_width * hidden_channels, n_neurons)),\n",
    "                    ('bn_ro', nn.BatchNorm1d(n_neurons, momentum=momentum)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    def initialize(self, std=None):\n",
    "        if std is None:\n",
    "            std = self.init_std\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, std=std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_core(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.readout(x)\n",
    "        return nn.functional.elu(x) + 1\n",
    "    \n",
    "    def regularizer(self):\n",
    "        return self.readout[0].weight.abs().sum() * self.gamma\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "cnn_model = CNN(input_height=64, input_width=36, n_neurons=5335)\n",
    "score, output, model_state = train_model(model=cnn_model, dataloader=dataloaders)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=======================================\n",
      "correlation -0.00026665637\n",
      "poisson_loss 3374573.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1: 100%|██████████| 75/75 [00:07<00:00,  9.73it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[001|00/05] ---> 0.021236207336187363\n",
      "=======================================\n",
      "correlation 0.021236207\n",
      "poisson_loss 2463599.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 2: 100%|██████████| 75/75 [00:05<00:00, 14.51it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[002|00/05] ---> 0.07052340358495712\n",
      "=======================================\n",
      "correlation 0.0705234\n",
      "poisson_loss 2113180.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 3: 100%|██████████| 75/75 [00:05<00:00, 14.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[003|00/05] ---> 0.12169219553470612\n",
      "=======================================\n",
      "correlation 0.121692196\n",
      "poisson_loss 1951988.6\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 4: 100%|██████████| 75/75 [00:05<00:00, 14.47it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[004|00/05] ---> 0.14366552233695984\n",
      "=======================================\n",
      "correlation 0.14366552\n",
      "poisson_loss 1886504.9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 5: 100%|██████████| 75/75 [00:05<00:00, 14.50it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[005|00/05] ---> 0.1598166525363922\n",
      "=======================================\n",
      "correlation 0.15981665\n",
      "poisson_loss 1853729.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 6: 100%|██████████| 75/75 [00:05<00:00, 14.48it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[006|00/05] ---> 0.17508751153945923\n",
      "=======================================\n",
      "correlation 0.17508751\n",
      "poisson_loss 1839959.4\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 7: 100%|██████████| 75/75 [00:05<00:00, 14.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[007|00/05] ---> 0.19102248549461365\n",
      "=======================================\n",
      "correlation 0.19102249\n",
      "poisson_loss 1814787.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 8: 100%|██████████| 75/75 [00:05<00:00, 14.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[008|01/05] -/-> 0.18706892430782318\n",
      "=======================================\n",
      "correlation 0.18706892\n",
      "poisson_loss 1824098.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 9: 100%|██████████| 75/75 [00:05<00:00, 14.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[009|01/05] ---> 0.20585818588733673\n",
      "=======================================\n",
      "correlation 0.20585819\n",
      "poisson_loss 1804795.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 10: 100%|██████████| 75/75 [00:05<00:00, 14.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[010|00/05] ---> 0.20773445069789886\n",
      "=======================================\n",
      "correlation 0.20773445\n",
      "poisson_loss 1802806.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 11: 100%|██████████| 75/75 [00:05<00:00, 14.47it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[011|01/05] -/-> 0.2031957507133484\n",
      "=======================================\n",
      "correlation 0.20319575\n",
      "poisson_loss 1812389.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 12: 100%|██████████| 75/75 [00:05<00:00, 14.46it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[012|02/05] -/-> 0.1954425573348999\n",
      "=======================================\n",
      "correlation 0.19544256\n",
      "poisson_loss 1843691.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 13: 100%|██████████| 75/75 [00:05<00:00, 14.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[013|03/05] -/-> 0.195707768201828\n",
      "=======================================\n",
      "correlation 0.19570777\n",
      "poisson_loss 1873333.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 14: 100%|██████████| 75/75 [00:05<00:00, 14.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[014|04/05] -/-> 0.18757301568984985\n",
      "=======================================\n",
      "correlation 0.18757302\n",
      "poisson_loss 1916169.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 15: 100%|██████████| 75/75 [00:05<00:00, 14.42it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[015|05/05] -/-> 0.17145362496376038\n",
      "Restoring best model after lr decay! 0.171454 ---> 0.207734\n",
      "=======================================\n",
      "correlation 0.20773445\n",
      "poisson_loss 1802806.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 16: 100%|██████████| 75/75 [00:05<00:00, 14.43it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    16: reducing learning rate of group 0 to 1.5000e-03.\n",
      "[016|01/05] -/-> 0.20370830595493317\n",
      "=======================================\n",
      "correlation 0.2037083\n",
      "poisson_loss 1818983.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 17: 100%|██████████| 75/75 [00:05<00:00, 14.43it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[017|02/05] -/-> 0.1997910439968109\n",
      "=======================================\n",
      "correlation 0.19979104\n",
      "poisson_loss 1815871.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 18: 100%|██████████| 75/75 [00:05<00:00, 14.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[018|03/05] -/-> 0.19749383628368378\n",
      "=======================================\n",
      "correlation 0.19749384\n",
      "poisson_loss 1829960.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 19: 100%|██████████| 75/75 [00:05<00:00, 14.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[019|04/05] -/-> 0.19291333854198456\n",
      "=======================================\n",
      "correlation 0.19291334\n",
      "poisson_loss 1849161.6\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 20: 100%|██████████| 75/75 [00:05<00:00, 14.42it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[020|05/05] -/-> 0.18776088953018188\n",
      "Restoring best model after lr decay! 0.187761 ---> 0.207734\n",
      "=======================================\n",
      "correlation 0.20773445\n",
      "poisson_loss 1802806.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 21: 100%|██████████| 75/75 [00:05<00:00, 14.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[021|01/05] -/-> 0.20450641214847565\n",
      "=======================================\n",
      "correlation 0.20450641\n",
      "poisson_loss 1801188.1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 22: 100%|██████████| 75/75 [00:05<00:00, 14.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    22: reducing learning rate of group 0 to 4.5000e-04.\n",
      "[022|02/05] -/-> 0.20180119574069977\n",
      "=======================================\n",
      "correlation 0.2018012\n",
      "poisson_loss 1814779.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 23: 100%|██████████| 75/75 [00:05<00:00, 14.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[023|03/05] -/-> 0.20084376633167267\n",
      "=======================================\n",
      "correlation 0.20084377\n",
      "poisson_loss 1816706.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 24: 100%|██████████| 75/75 [00:05<00:00, 14.37it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[024|04/05] -/-> 0.19823312759399414\n",
      "=======================================\n",
      "correlation 0.19823313\n",
      "poisson_loss 1823781.9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 25: 100%|██████████| 75/75 [00:05<00:00, 14.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[025|05/05] -/-> 0.1976805031299591\n",
      "Restoring best model after lr decay! 0.197681 ---> 0.207734\n",
      "Restoring best model! 0.207734 ---> 0.207734\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trying out the State-of-the-Art (SOTA) model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we got some sense on how we could go about training linear and nonlinear network models to predict V1 neuron responses to natural images, and we just saw how the nonlinear network seems to bring significant improvement to the performance beyond the LN network.\n",
    "\n",
    "You might now be wondering, how good can we get? To get a sense of this, let's go ahead and train a state-of-the-art (SOTA) network model for mouse V1 responses to natual images as published in our recent work in [Lurz et al. ICLR 2021](https://github.com/sinzlab/Lurz_2020_code).\n",
    "\n",
    "To keep things simple, I have provided for the network implementation in the `lviv` package, so you can build the model just by invoking a function!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from lviv.models import build_lurz2020_model\n",
    "model_config = {'init_mu_range': 0.55,\n",
    "                'init_sigma': 0.4,\n",
    "                'input_kern': 15,\n",
    "                'hidden_kern': 13,\n",
    "                'gamma_input': 1.0,\n",
    "                'grid_mean_predictor': {'type': 'cortex',\n",
    "                                        'input_dimensions': 2,\n",
    "                                        'hidden_layers': 0,\n",
    "                                        'hidden_features': 0,\n",
    "                                        'final_tanh': False},\n",
    "                'gamma_readout': 2.439\n",
    "               }\n",
    "\n",
    "sota_model = build_lurz2020_model(**model_config, dataloaders=dataloaders, seed=1234)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "score, output, model_state = train_model(model=sota_model, dataloader=dataloaders)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=======================================\n",
      "correlation 0.00034036028\n",
      "poisson_loss 3467926.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1: 100%|██████████| 75/75 [00:03<00:00, 24.61it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[001|00/05] ---> 0.06801845878362656\n",
      "=======================================\n",
      "correlation 0.06801846\n",
      "poisson_loss 1933881.6\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 2: 100%|██████████| 75/75 [00:03<00:00, 24.70it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[002|00/05] ---> 0.0993238165974617\n",
      "=======================================\n",
      "correlation 0.09932382\n",
      "poisson_loss 1907383.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 3: 100%|██████████| 75/75 [00:03<00:00, 24.73it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[003|00/05] ---> 0.13521890342235565\n",
      "=======================================\n",
      "correlation 0.1352189\n",
      "poisson_loss 1867068.4\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 4: 100%|██████████| 75/75 [00:03<00:00, 24.61it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[004|00/05] ---> 0.1598539799451828\n",
      "=======================================\n",
      "correlation 0.15985398\n",
      "poisson_loss 1841975.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 5: 100%|██████████| 75/75 [00:03<00:00, 24.66it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[005|00/05] ---> 0.18217037618160248\n",
      "=======================================\n",
      "correlation 0.18217038\n",
      "poisson_loss 1817149.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 6: 100%|██████████| 75/75 [00:03<00:00, 24.61it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[006|00/05] ---> 0.20668023824691772\n",
      "=======================================\n",
      "correlation 0.20668024\n",
      "poisson_loss 1790282.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 7: 100%|██████████| 75/75 [00:03<00:00, 24.64it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[007|00/05] ---> 0.22378003597259521\n",
      "=======================================\n",
      "correlation 0.22378004\n",
      "poisson_loss 1772537.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 8: 100%|██████████| 75/75 [00:03<00:00, 24.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[008|00/05] ---> 0.2300778031349182\n",
      "=======================================\n",
      "correlation 0.2300778\n",
      "poisson_loss 1773524.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 9: 100%|██████████| 75/75 [00:03<00:00, 24.62it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[009|00/05] ---> 0.24510174989700317\n",
      "=======================================\n",
      "correlation 0.24510175\n",
      "poisson_loss 1747724.4\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 10: 100%|██████████| 75/75 [00:03<00:00, 24.58it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[010|00/05] ---> 0.25031936168670654\n",
      "=======================================\n",
      "correlation 0.25031936\n",
      "poisson_loss 1738287.1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 11: 100%|██████████| 75/75 [00:03<00:00, 24.54it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[011|00/05] ---> 0.263248085975647\n",
      "=======================================\n",
      "correlation 0.2632481\n",
      "poisson_loss 1725767.1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 12: 100%|██████████| 75/75 [00:03<00:00, 24.56it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[012|01/05] -/-> 0.26304689049720764\n",
      "=======================================\n",
      "correlation 0.2630469\n",
      "poisson_loss 1730026.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 13: 100%|██████████| 75/75 [00:03<00:00, 24.26it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[013|01/05] ---> 0.2683936059474945\n",
      "=======================================\n",
      "correlation 0.2683936\n",
      "poisson_loss 1716664.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 14: 100%|██████████| 75/75 [00:03<00:00, 24.50it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[014|00/05] ---> 0.27154725790023804\n",
      "=======================================\n",
      "correlation 0.27154726\n",
      "poisson_loss 1719069.1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 15: 100%|██████████| 75/75 [00:03<00:00, 24.54it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[015|00/05] ---> 0.27546003460884094\n",
      "=======================================\n",
      "correlation 0.27546003\n",
      "poisson_loss 1706879.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 16: 100%|██████████| 75/75 [00:03<00:00, 24.56it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[016|00/05] ---> 0.2800818681716919\n",
      "=======================================\n",
      "correlation 0.28008187\n",
      "poisson_loss 1706023.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 17: 100%|██████████| 75/75 [00:03<00:00, 24.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[017|01/05] -/-> 0.2778778076171875\n",
      "=======================================\n",
      "correlation 0.2778778\n",
      "poisson_loss 1704340.9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 18: 100%|██████████| 75/75 [00:03<00:00, 24.46it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[018|01/05] ---> 0.28181901574134827\n",
      "=======================================\n",
      "correlation 0.28181902\n",
      "poisson_loss 1699057.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 19: 100%|██████████| 75/75 [00:03<00:00, 24.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[019|00/05] ---> 0.2833921015262604\n",
      "=======================================\n",
      "correlation 0.2833921\n",
      "poisson_loss 1700085.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 20: 100%|██████████| 75/75 [00:03<00:00, 24.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[020|01/05] -/-> 0.28238430619239807\n",
      "=======================================\n",
      "correlation 0.2823843\n",
      "poisson_loss 1703407.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 21: 100%|██████████| 75/75 [00:03<00:00, 24.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[021|01/05] ---> 0.2864673137664795\n",
      "=======================================\n",
      "correlation 0.2864673\n",
      "poisson_loss 1695202.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 22: 100%|██████████| 75/75 [00:03<00:00, 24.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[022|01/05] -/-> 0.28594663739204407\n",
      "=======================================\n",
      "correlation 0.28594664\n",
      "poisson_loss 1698530.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 23: 100%|██████████| 75/75 [00:03<00:00, 24.42it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[023|01/05] ---> 0.2886951267719269\n",
      "=======================================\n",
      "correlation 0.28869513\n",
      "poisson_loss 1692419.4\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 24: 100%|██████████| 75/75 [00:03<00:00, 24.42it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[024|01/05] -/-> 0.2807021737098694\n",
      "=======================================\n",
      "correlation 0.28070217\n",
      "poisson_loss 1711856.9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 25: 100%|██████████| 75/75 [00:03<00:00, 24.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[025|02/05] -/-> 0.2867578864097595\n",
      "=======================================\n",
      "correlation 0.2867579\n",
      "poisson_loss 1700438.6\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 26: 100%|██████████| 75/75 [00:03<00:00, 24.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[026|02/05] ---> 0.2890879809856415\n",
      "=======================================\n",
      "correlation 0.28908798\n",
      "poisson_loss 1693386.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 27: 100%|██████████| 75/75 [00:03<00:00, 24.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[027|01/05] -/-> 0.2860674262046814\n",
      "=======================================\n",
      "correlation 0.28606743\n",
      "poisson_loss 1698096.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 28: 100%|██████████| 75/75 [00:03<00:00, 24.40it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[028|01/05] ---> 0.2897437810897827\n",
      "=======================================\n",
      "correlation 0.28974378\n",
      "poisson_loss 1692401.6\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 29: 100%|██████████| 75/75 [00:03<00:00, 24.42it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[029|01/05] -/-> 0.2884751558303833\n",
      "=======================================\n",
      "correlation 0.28847516\n",
      "poisson_loss 1694695.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 30: 100%|██████████| 75/75 [00:03<00:00, 24.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[030|02/05] -/-> 0.286041796207428\n",
      "=======================================\n",
      "correlation 0.2860418\n",
      "poisson_loss 1698880.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 31: 100%|██████████| 75/75 [00:03<00:00, 24.40it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[031|03/05] -/-> 0.2875349819660187\n",
      "=======================================\n",
      "correlation 0.28753498\n",
      "poisson_loss 1699648.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 32: 100%|██████████| 75/75 [00:03<00:00, 24.38it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[032|03/05] ---> 0.29187992215156555\n",
      "=======================================\n",
      "correlation 0.29187992\n",
      "poisson_loss 1687671.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 33: 100%|██████████| 75/75 [00:03<00:00, 24.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[033|01/05] -/-> 0.28936630487442017\n",
      "=======================================\n",
      "correlation 0.2893663\n",
      "poisson_loss 1691776.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 34: 100%|██████████| 75/75 [00:03<00:00, 24.46it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[034|02/05] -/-> 0.2871137261390686\n",
      "=======================================\n",
      "correlation 0.28711373\n",
      "poisson_loss 1693849.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 35: 100%|██████████| 75/75 [00:03<00:00, 24.18it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[035|03/05] -/-> 0.2877572774887085\n",
      "=======================================\n",
      "correlation 0.28775728\n",
      "poisson_loss 1697180.1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 36: 100%|██████████| 75/75 [00:03<00:00, 24.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[036|04/05] -/-> 0.29029035568237305\n",
      "=======================================\n",
      "correlation 0.29029036\n",
      "poisson_loss 1694217.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 37: 100%|██████████| 75/75 [00:03<00:00, 24.38it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[037|05/05] -/-> 0.2860797047615051\n",
      "Restoring best model after lr decay! 0.286080 ---> 0.291880\n",
      "=======================================\n",
      "correlation 0.29187992\n",
      "poisson_loss 1687671.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 38: 100%|██████████| 75/75 [00:03<00:00, 24.41it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    38: reducing learning rate of group 0 to 1.5000e-03.\n",
      "[038|01/05] -/-> 0.28654494881629944\n",
      "=======================================\n",
      "correlation 0.28654495\n",
      "poisson_loss 1698977.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 39: 100%|██████████| 75/75 [00:03<00:00, 24.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[039|01/05] ---> 0.29677826166152954\n",
      "=======================================\n",
      "correlation 0.29677826\n",
      "poisson_loss 1677751.9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 40: 100%|██████████| 75/75 [00:03<00:00, 24.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[040|01/05] -/-> 0.2955741584300995\n",
      "=======================================\n",
      "correlation 0.29557416\n",
      "poisson_loss 1679841.9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 41: 100%|██████████| 75/75 [00:03<00:00, 24.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[041|02/05] -/-> 0.2957981526851654\n",
      "=======================================\n",
      "correlation 0.29579815\n",
      "poisson_loss 1680762.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 42: 100%|██████████| 75/75 [00:03<00:00, 24.40it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[042|03/05] -/-> 0.2918468117713928\n",
      "=======================================\n",
      "correlation 0.2918468\n",
      "poisson_loss 1694253.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 43: 100%|██████████| 75/75 [00:03<00:00, 24.43it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[043|04/05] -/-> 0.2910313010215759\n",
      "=======================================\n",
      "correlation 0.2910313\n",
      "poisson_loss 1692557.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 44: 100%|██████████| 75/75 [00:03<00:00, 24.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[044|05/05] -/-> 0.2915862202644348\n",
      "Restoring best model after lr decay! 0.291586 ---> 0.296778\n",
      "=======================================\n",
      "correlation 0.29677826\n",
      "poisson_loss 1677751.9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 45: 100%|██████████| 75/75 [00:03<00:00, 24.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    45: reducing learning rate of group 0 to 4.5000e-04.\n",
      "[045|01/05] -/-> 0.29598677158355713\n",
      "=======================================\n",
      "correlation 0.29598677\n",
      "poisson_loss 1680192.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 46: 100%|██████████| 75/75 [00:03<00:00, 24.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[046|01/05] ---> 0.296872079372406\n",
      "=======================================\n",
      "correlation 0.29687208\n",
      "poisson_loss 1679943.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 47: 100%|██████████| 75/75 [00:03<00:00, 24.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[047|01/05] -/-> 0.29609593749046326\n",
      "=======================================\n",
      "correlation 0.29609594\n",
      "poisson_loss 1682917.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 48: 100%|██████████| 75/75 [00:03<00:00, 24.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[048|02/05] -/-> 0.2934490740299225\n",
      "=======================================\n",
      "correlation 0.29344907\n",
      "poisson_loss 1690523.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 49: 100%|██████████| 75/75 [00:03<00:00, 24.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[049|03/05] -/-> 0.29546308517456055\n",
      "=======================================\n",
      "correlation 0.2954631\n",
      "poisson_loss 1680584.9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 50: 100%|██████████| 75/75 [00:03<00:00, 24.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[050|04/05] -/-> 0.29319944977760315\n",
      "=======================================\n",
      "correlation 0.29319945\n",
      "poisson_loss 1687866.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 51: 100%|██████████| 75/75 [00:03<00:00, 24.22it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[051|05/05] -/-> 0.2939625084400177\n",
      "Restoring best model after lr decay! 0.293963 ---> 0.296872\n",
      "Restoring best model! 0.296872 ---> 0.296872\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It turns out that we can have *linearized* version of the SOTA model. This effectively removes all nonlinear operations within the network except for the very last nonlinear activation, rendering the network into a **LN model** but with more complex architecture."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "linear_model_config = dict(model_config) # copy the config\n",
    "linear_model_config['linear'] = True # set linear to True to make it a LN model!\n",
    "\n",
    "sota_ln_model = build_lurz2020_model(**linear_model_config, dataloaders=dataloaders, seed=1234)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score, output, model_state = train_model(model=sota_model, dataloader=dataloaders)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyzing the trained model to gain insights into the brain"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color='green'>\n",
    "    NOTE to collaborators: \n",
    "    Please provide code for generating gradient receptive field and MEI for the sota networks. By this point, they should have `sota_model` and `sota_ln_model` corresponding to the best nonlinear and linear model based on the model architecture as found in Lurz et al. 2021.\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IVIV_2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}