{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
    "<a target=\"_blank\"  href=\"https://colab.research.google.com/github/eywalker/LVIV-2021/blob/main/notebooks/DeepLearing%20in%20Neuroscience.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
    "</td><td>\n",
    "<a target=\"_blank\"  href=\"https://github.com/eywalker/LVIV-2021/blob/main/notebooks/DeepLearing%20in%20Neuroscience.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp0SO4gr2D7w"
   },
   "source": [
    "# Welcome to Deep Learning in Neuroscience by Edgar Y. Walker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Jupyter notebook to accompany the course on \"Deep Learning in Neuroscience\" taught as part of the Lviv Data Science Summer School 2021. This notebook as well as any other relevant information can be found in the [GitHub repository](https://github.com/eywalker/lviv-2021)!\n",
    "\n",
    "In this course, we will learn how deep learning is getting utilized in studying neuroscience, specifically in building models of neurons to complex sensory inputs such as natural images. We will start by going through some neuroscience primer. We will then get our hands dirty by taking real neuronal responses recorded from mouse primary visual cortex (V1) as the mouse observes a bunch of natural images and developing a model to predict these responses. By the end of this course, you will gain some basic familiarity in utilizing deep learning models to predict responses of 1000s of neurons to natural images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>NOTE: Please run this section at the very beginning of the first session!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to dive in and learn how deep learning is used in neuroscience and get your first neural predictive model trained, we need to install some prerequisite packages and download some neuronal data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to primarily use [PyTorch](https://pytorch.org) to build, train and evaluate our deep learning models and I am going to assume some familiarity with PyTorch already.\n",
    "\n",
    "Also to be able to handle the dataset containing neuronal activities, we are going to make our life easier by using a few existing libraries. I have prepared a library called [lviv2021](https://github.com/eywalker/lviv2021). This library has a dependency on [neuralpredictors](https://github.com/sinzlab/neuralpredictors), which is a collection of PyTorch layers, tools and other utilities that would prove helpful to train networks to predict neuronal responses.\n",
    "\n",
    "Let's go ahead and install this inside the Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "m7dJ_i-tSPEq",
    "outputId": "bdc3f3f8-e451-4bd3-83d5-3f126b88f431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.9.0+cu102\n",
      "  Downloading https://download.pytorch.org/whl/cu102/torch-1.9.0%2Bcu102-cp38-cp38-linux_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |███████▍           ^C           | 192.8 MB 48.8 MB/s eta 0:00:14    |▉                               | 22.9 MB 38.0 MB/s eta 0:00:22     |██                              | 53.8 MB 42.0 MB/s eta 0:00:19\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 228, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 182, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 323, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/legacy/resolver.py\", line 183, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/legacy/resolver.py\", line 388, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/resolution/legacy/resolver.py\", line 340, in _get_abstract_dist_for\n",
      "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/prepare.py\", line 467, in prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/prepare.py\", line 255, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/prepare.py\", line 129, in get_http_url\n",
      "    from_path, content_type = _download_http_url(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/prepare.py\", line 282, in _download_http_url\n",
      "    for chunk in download.chunks:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/progress_bars.py\", line 172, in iter\n",
      "    self.next(len(x))  # noqa: B305\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/progress/__init__.py\", line 120, in next\n",
      "    self.update()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/progress/bar.py\", line 83, in update\n",
      "    self.writeln(line)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/progress/__init__.py\", line 101, in writeln\n",
      "    self.clearln()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/progress/__init__.py\", line 90, in clearln\n",
      "    print('\\r\\x1b[K', end='', file=self.file)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/progress_bars.py\", line 115, in handle_sigint\n",
      "    self.finish()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/progress_bars.py\", line 105, in finish\n",
      "    super(InterruptibleMixin, self).finish()  # type: ignore\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/progress/__init__.py\", line 107, in finish\n",
      "    print(file=self.file)\n",
      "RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[KCollecting git+https://github.com/eywalker/lviv-2021.git\n",
      "  Cloning https://github.com/eywalker/lviv-2021.git to /tmp/pip-req-build-uzvv_cgp\n",
      "Requirement already satisfied: neuralpredictors~=0.0.1 in /usr/local/lib/python3.8/dist-packages (from lviv==0.1) (0.0.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from lviv==0.1) (1.7.0+cu110)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from lviv==0.1) (1.19.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from neuralpredictors~=0.0.1->lviv==0.1) (4.51.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from neuralpredictors~=0.0.1->lviv==0.1) (3.1.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from neuralpredictors~=0.0.1->lviv==0.1) (1.1.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->lviv==0.1) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.8/dist-packages (from torch->lviv==0.1) (0.6)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from torch->lviv==0.1) (0.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->neuralpredictors~=0.0.1->lviv==0.1) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/dist-packages (from pandas->neuralpredictors~=0.0.1->lviv==0.1) (2020.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas->neuralpredictors~=0.0.1->lviv==0.1) (1.11.0)\n",
      "Building wheels for collected packages: lviv\n",
      "  Building wheel for lviv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lviv: filename=lviv-0.1-py3-none-any.whl size=15751 sha256=3cdcb3fef126076d663084a942f96eb43f74aa1a27c203354da11ceb4637f2ce\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3angtku2/wheels/80/e1/15/43cc30475a2ac297d60fbcd8bc9f8acb7b5d56229c85ec903f\n",
      "Successfully built lviv\n",
      "Installing collected packages: lviv\n",
      "Successfully installed lviv-0.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch dependency\n",
    "!pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    \n",
    "# Install \n",
    "!pip3 install git+https://github.com/eywalker/lviv-2021.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the dataset made available for our recent paper [Lurz et al. ICLR 2021](https://github.com/sinzlab/Lurz_2020_code), predicting responses of mouse visual cortex to natural images. \n",
    "\n",
    "The dataset can take anywhere from 5-10 min to download, so please be sure to **run the following at the very beginning of the session!** We are going to first spend some time learning the basics of computational neuroscience in the study of system identification. It would be best that you let the download take place while we go over the neursocience primer so that it will be ready when we come back here to get our hands dirty!\n",
    "\n",
    "To download the data, simply execute the following cell, and let it run till completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsNehEZlTSL4",
    "outputId": "8e2984ad-9f32-4dfd-b8b7-4536f2e53823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/data’: File exists\n",
      "Cloning into '/data'...\n",
      "remote: Enumerating objects: 23150, done.\u001b[K                                            \u001b[K\n",
      "remote: Counting objects: 100% (23150/23150), done.\u001b[K                                  \u001b[K\n",
      "remote: Compressing objects: 100% (11714/11714), done.                                  \u001b[K\n",
      "remote: Total 23150 (delta 11436), reused 23134 (delta 11434)\u001b[K    \n",
      "Receiving objects: 100% (23150/23150), 252.05 MiB | 506.00 KiB/s, done.\n",
      "Resolving deltas: 100% (11436/11436), done.\n",
      "Checking out files: 100% (24035/24035), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://gin.g-node.org/cajal/Lurz2020.git /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing models of neural population responses to natural images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have been primed with just enough background neuroscience, let's get our hand dirty and try to build our first neural predictive models.\n",
    "\n",
    "As part of the setup, we have downloaded a 2-photon imaging dataset from mouse primary visual cortex as we present 1000s of natural images (if not done yet, please do so immediately by stepping through the beginning sections of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating the neuroscience data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any data science project, you must start by understanding your data! Let's take some time to navigate the data you downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change.log  config.json  \u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mmeta\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ./data/static20457-5-9-preproc0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbehavior\u001b[0m/  \u001b[01;34mimages\u001b[0m/  \u001b[01;34mpupil_center\u001b[0m/  \u001b[01;34mresponses\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ./data/static20457-5-9-preproc0/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.npy\n",
      "1.npy\n",
      "10.npy\n",
      "100.npy\n",
      "1000.npy\n",
      "1001.npy\n",
      "1002.npy\n",
      "1003.npy\n",
      "1004.npy\n",
      "1005.npy\n",
      "1006.npy\n",
      "1007.npy\n",
      "1008.npy\n",
      "1009.npy\n",
      "101.npy\n",
      "1010.npy\n",
      "1011.npy\n",
      "1012.npy\n",
      "1013.npy\n",
      "1014.npy\n",
      "1015.npy\n",
      "1016.npy\n",
      "1017.npy\n",
      "1018.npy\n",
      "1019.npy\n",
      "102.npy\n",
      "1020.npy\n",
      "1021.npy\n",
      "1022.npy\n",
      "1023.npy\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "ls ./data/static20457-5-9-preproc0/data/responses | head -30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.npy\n",
      "1.npy\n",
      "10.npy\n",
      "100.npy\n",
      "1000.npy\n",
      "1001.npy\n",
      "1002.npy\n",
      "1003.npy\n",
      "1004.npy\n",
      "1005.npy\n",
      "1006.npy\n",
      "1007.npy\n",
      "1008.npy\n",
      "1009.npy\n",
      "101.npy\n",
      "1010.npy\n",
      "1011.npy\n",
      "1012.npy\n",
      "1013.npy\n",
      "1014.npy\n",
      "1015.npy\n",
      "1016.npy\n",
      "1017.npy\n",
      "1018.npy\n",
      "1019.npy\n",
      "102.npy\n",
      "1020.npy\n",
      "1021.npy\n",
      "1022.npy\n",
      "1023.npy\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "ls ./data/static20457-5-9-preproc0/data/images | head -30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that both responses and contained in collections of `numpy` files named like `1.npy` or `31.npy`. The number here corresponds to a specific **trial** or simply different image presentation during an experiment.\n",
    "\n",
    "Let's take a look at some of these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data files one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick some trial and load the image as well as the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_idx = 1100\n",
    "trial_image = np.load(f'./data/static20457-5-9-preproc0/data/images/{trial_idx}.npy')\n",
    "trial_responses = np.load(f'./data/static20457-5-9-preproc0/data/responses/{trial_idx}.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is shaped as $\\text{channel} \\times \\text{height} \\times \\text{width}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 36, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 35.5, -0.5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trial_image.squeeze(), cmap='gray', vmin=0, vmax=255)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, the shape of `trial_response` is simply the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5335,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_responses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.409307320414307e-10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_responses.min() # responses are practically always >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.39189366955226"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_responses.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, dpi=150)\n",
    "ax.hist(trial_responses, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see most neuron's responses stay very close to 0 - signifying no activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can inspect the image and the corresponding neural population responses one image at a time, this is quite cumbersome and also impractical for use in network training. Fortunately, the `lviv` package provides us with a convenience function that will help to load the entire dataset as PyTorch dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lviv.dataset import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading, we get to specify the batchsize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-df5eb9322292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/data/static20457-5-9-preproc0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lviv/dataset.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, batch_size, areas, layers, tier, neuron_ids, neuron_n, exclude_neuron_n, neuron_base_seed, image_ids, image_n, image_base_seed, get_key, cuda, normalize, exclude, return_test_sampler, oracle_condition, seed)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0mit\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata_key\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mfollowed\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdataloder\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     assert any(\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mimage_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_n\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_base_seed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lviv/utility/nn_helper.py\u001b[0m in \u001b[0;36mset_random_seed\u001b[0;34m(seed, deterministic)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# this sets both CPU and CUDA seeds for PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_io_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;36m0xffff_ffff_ffff_ffff\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "dataloaders = load_dataset(path = '/content/data/static20457-5-9-preproc0', batch_size=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a dictionary consistent of three dataloaders for training, validation, and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar inspection can be done on the **validation** and **testing** dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of images in validation set\n",
    "len(dataloaders['validation'].sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of images in test set\n",
    "len(dataloaders['test'].sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might think that we have a lot of images in test set, but this is because test set consists of repeated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional trial information can be observed by accessing the underlying PyTorch dataset object and looking at the `trial_info`. Note that this is not part of the standard PyTorch dataset/dataloader interface, but rather a feature specifically provided by the library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access to the dataset object that underlies all dataloaders\n",
    "testset = dataloaders['test'].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trials = np.where(testset.trial_info['tiers'] == 'test')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = testset.trial_info['frame_image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 104,  128,  183,  355,  479,  483,  656,  803,  830,  936, 1201,\n",
       "       1494, 1596, 1652, 1656, 1682, 1731, 1756, 1796, 2005, 2008, 2014,\n",
       "       2159, 2214, 2389, 2586, 2710, 2746, 2747, 2803, 2816, 2825, 2954,\n",
       "       3018, 3107, 3144, 3163, 3372, 3427, 3438, 3487, 3507, 3562, 3702,\n",
       "       3847, 3924, 4231, 4295, 4373, 4397, 4400, 4430, 4594, 4619, 4667,\n",
       "       4674, 4717, 4739, 4782, 4812, 4814, 4821, 4923, 4953, 5034, 5128,\n",
       "       5166, 5225, 5264, 5288, 5322, 5334, 5399, 5402, 5504, 5640, 5671,\n",
       "       5679, 5754, 5782, 6013, 6034, 6066, 6082, 6205, 6238, 6248, 6490,\n",
       "       6562, 6773, 6790, 6831, 6886, 7017, 7028, 7107, 7119, 7120, 7154,\n",
       "       7495])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(image_ids[test_trials])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(image_ids[test_trials]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see that the test set consists of 100 unique images, each repeated up to 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frame_trial_ts',\n",
       " 'animal_id',\n",
       " 'session',\n",
       " 'frame_pre_blank_period',\n",
       " 'frame_last_flip',\n",
       " 'trial_idx',\n",
       " 'frame_presentation_time',\n",
       " 'frame_image_class',\n",
       " 'frame_image_id',\n",
       " 'scan_idx',\n",
       " 'tiers',\n",
       " 'condition_hash']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.trial_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1301, 5927, 3982, ...,  464,  819, 3025])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.trial_info.frame_image_id  # gives information about presented image ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5993"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloaders['validation'].dataset.trial_info.frame_image_id)  # gives information about presented image ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling the neuronal responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully loaded the dataset and inspected its contents, it's time for us to start **modeling** the responses.\n",
    "\n",
    "We will start by building a very basic **Linear-Nonlinear model** - which is nothing more than a shallow neural network with single linear layer followed by an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcVMbaG9xXNY"
   },
   "source": [
    "## Linear-Nonlinear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_height,\n",
    "        input_width,\n",
    "        n_neurons,\n",
    "        momentum=0.1,\n",
    "        init_std=1e-3,\n",
    "        gamma=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(1, momentum=momentum, affine=False)\n",
    "        self.linear = nn.Linear(input_height * input_width, n_neurons)\n",
    "        self.gamma = gamma\n",
    "        self.init_std = init_std\n",
    "        self.initialize()\n",
    "        \n",
    "\n",
    "    def initialize(self, std=None):\n",
    "        if std is None:\n",
    "            std = self.init_std\n",
    "        nn.init.normal_(self.linear.weight.data, std=std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.linear(x.flatten(1))\n",
    "        return nn.functional.elu(x) + 1\n",
    "\n",
    "    def regularizer(self):\n",
    "        return self.gamma * self.linear.weight.abs().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0505, device='cuda:0', grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "EWnBvqRMM2VH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from typing import Iterable\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_height,\n",
    "        input_width,\n",
    "        n_neurons,\n",
    "        input_channels,\n",
    "        hidden_channels,\n",
    "        input_kern,\n",
    "        hidden_kern,\n",
    "        layers=3,\n",
    "        momentum=0.1,\n",
    "        pad_input=True,\n",
    "        batch_norm=True,\n",
    "        hidden_dilation=1,\n",
    "        linear=False,\n",
    "        readout='fc',\n",
    "        gamma=0\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_channels:     Integer, number of input channels as in\n",
    "            hidden_channels:    Number of hidden channels (i.e feature maps) in each hidden layer\n",
    "            input_kern:     kernel size of the first layer (i.e. the input layer)\n",
    "            hidden_kern:    kernel size of each hidden layer's kernel\n",
    "            layers:         number of layers\n",
    "            momentum:       BN momentum\n",
    "            pad_input:      Boolean, if True, applies zero padding to all convolutions\n",
    "            batch_norm:     Boolean, if True appends a BN layer after each convolutional layer\n",
    "            hidden_dilation:   If set to > 1, will apply dilated convs for all hidden layers\n",
    "            linear:         Boolean, if True, remove all nonlinearity in the model\n",
    "            readout:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.core = nn.Sequential()\n",
    "        self.readout_type = readout\n",
    "        self.n_neurons = n_neurons\n",
    "        self.gamma = gamma\n",
    "\n",
    "\n",
    "        # Core: --- first layer\n",
    "        layer = OrderedDict()\n",
    "        layer[\"conv\"] = nn.Conv2d(\n",
    "            input_channels, hidden_channels, input_kern, padding=input_kern // 2 if pad_input else 0, bias=False\n",
    "        )\n",
    "        if batch_norm:\n",
    "            layer[\"norm\"] = nn.BatchNorm2d(hidden_channels, momentum=momentum)\n",
    "        if not linear:\n",
    "            layer[\"nonlin\"] = nn.ELU(inplace=True)\n",
    "        self.core.add_module(\"layer0\", nn.Sequential(layer))\n",
    "        if not isinstance(hidden_kern, Iterable):\n",
    "            hidden_kern = [hidden_kern] * (self.layers - 1)\n",
    "\n",
    "        # Core: --- other layers\n",
    "        for l in range(1, self.layers):\n",
    "            layer = OrderedDict()\n",
    "            hidden_padding = ((hidden_kern[l - 1] - 1) * hidden_dilation + 1) // 2\n",
    "            layer[\"conv\"] = nn.Conv2d(\n",
    "                hidden_channels,\n",
    "                hidden_channels,\n",
    "                hidden_kern[l - 1],\n",
    "                padding=hidden_padding,\n",
    "                bias=False,\n",
    "                dilation=hidden_dilation,\n",
    "            )\n",
    "            if batch_norm:\n",
    "                layer[\"norm\"] = nn.BatchNorm2d(hidden_channels, momentum=momentum)\n",
    "            if not linear:\n",
    "                layer[\"nonlin\"] = nn.ELU(inplace=True)\n",
    "            self.core.add_module(\"layer{}\".format(l), nn.Sequential(layer))\n",
    "        self.apply(self.init_conv)\n",
    "        \n",
    "        # Readout\n",
    "        ## fully connected readout\n",
    "        if readout == 'fc':\n",
    "            in_dim = self.core_channels * input_height * input_width\n",
    "            self.readout = nn.Linear(in_dim, n_neurons, bias=True)\n",
    "        ## spatial separable readout\n",
    "        elif readout == 'spatial':\n",
    "            self.spatial = nn.Parameter(torch.Tensor(n_neurons, input_width, input_height))\n",
    "            self.features = nn.Parameter(torch.Tensor(n_neurons, self.core_channels))\n",
    "            self.readout_bias = nn.Parameter(torch.Tensor(n_neurons))\n",
    "            \n",
    "    def forward(self, input_):\n",
    "        ret = []\n",
    "        for l, feat in enumerate(self.core):\n",
    "            input_ = feat(input_)\n",
    "            ret.append(input_)\n",
    "        core_out = torch.cat(ret, dim=1)\n",
    "        \n",
    "        if self.readout_type == 'fc':\n",
    "            core_out = core_out.view([core_out.shape[0], -1])\n",
    "            readout_out = nn.functional.elu(self.readout(core_out)) + 1\n",
    "        elif self.readout_type == 'spatial':\n",
    "            readout_out = torch.einsum(\"ncwh,owh->nco\", core_out, self.spatial)\n",
    "            readout_out = torch.einsum(\"nco,oc->no\", readout_out, self.features)\n",
    "            readout_out = readout_out + self.readout_bias\n",
    "            readout_out = nn.functional.elu(readout_out) + 1\n",
    "        return readout_out\n",
    "\n",
    "    def regularizer(self):\n",
    "        if self.readout_type == 'fc':\n",
    "            return self.readout.weight.abs().sum() * self.gamma\n",
    "        elif self.readout_type == 'spatial':\n",
    "            return (\n",
    "            self.spatial.view(self.n_neurons, -1).abs().sum(dim=1, keepdim=True)\n",
    "            * self.features.view(self.n_neurons, -1).abs().sum(dim=1)\n",
    "        ).sum() * self.gamma\n",
    "\n",
    "    @property\n",
    "    def core_channels(self):\n",
    "        return len(self.core) * self.hidden_channels\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_conv(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0)\n",
    "                \n",
    "    def initialize_readout(self):\n",
    "        if self.readout_type == 'full':\n",
    "            nn.init.xavier_normal_(self.readout.weight.data)\n",
    "            self.readout.weight.data.fill_(0)\n",
    "        elif self.readout_type == 'spatial':\n",
    "            self.spatial.data.normal_(0, 1e-3)\n",
    "            self.features.data.normal_(0, 1e-3)\n",
    "            if self.readout_bias is not None:\n",
    "                self.readout_bias.data.fill_(0)\n",
    "                \n",
    "    def initialize(self):\n",
    "        self.core.apply(self.init_conv)\n",
    "        self.initialize_readout()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "B_OlswuSEs5O",
    "outputId": "12569a6b-e295-4d0c-a4ba-ae361191bff6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "IuhvIccOM7hu",
    "outputId": "eb1e3ea7-3c12-4164-c190-63fd2c42dcb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (core): Sequential(\n",
       "    (layer0): Sequential(\n",
       "      (conv): Conv2d(1, 64, kernel_size=(15, 15), stride=(1, 1), padding=(7, 7), bias=False)\n",
       "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (nonlin): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(13, 13), stride=(1, 1), padding=(6, 6), bias=False)\n",
       "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (nonlin): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(13, 13), stride=(1, 1), padding=(6, 6), bias=False)\n",
       "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (nonlin): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(13, 13), stride=(1, 1), padding=(6, 6), bias=False)\n",
       "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (nonlin): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_n = 5335\n",
    "model_config = {\n",
    "    'input_height': 64,\n",
    "    'input_width': 36,\n",
    "    'n_neurons': neuron_n,\n",
    "    'input_channels': 1,\n",
    "    'hidden_channels': 64,\n",
    "    'input_kern': 15,\n",
    "    'hidden_kern': 13,\n",
    "    'layers': 4,\n",
    "    'readout': 'spatial',\n",
    "    'gamma': 0.001,\n",
    "}\n",
    "model = CNN(**model_config)\n",
    "model.initialize()\n",
    "\n",
    "# be sure to place it on the GPU!\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "W0P2kOwOQ4ir",
    "outputId": "f3e44885-fa52-4b29-c355-55df1cbeb95d"
   },
   "outputs": [],
   "source": [
    "from lviv.models import build_lurz2020_model\n",
    "model_config = {'init_mu_range': 0.55,\n",
    "                'init_sigma': 0.4,\n",
    "                'input_kern': 15,\n",
    "                'hidden_kern': 13,\n",
    "                'gamma_input': 1.0,\n",
    "                'grid_mean_predictor': {'type': 'cortex',\n",
    "                                        'input_dimensions': 2,\n",
    "                                        'hidden_layers': 0,\n",
    "                                        'hidden_features': 0,\n",
    "                                        'final_tanh': False},\n",
    "                'gamma_readout': 2.439,\n",
    "                'linear': False}\n",
    "\n",
    "model = build_lurz2020_model(**model_config, dataloaders=dataloaders, seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bvy7NQZxbC4"
   },
   "source": [
    "## Build trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "O5vtSRWsRAbY"
   },
   "outputs": [],
   "source": [
    "from lviv.trainers import train_model\n",
    "\n",
    "trainer_config = {'track_training': True,\n",
    "                  'detach_core': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq1QwgXdxd6s"
   },
   "source": [
    "## Model training\n",
    "Approximate training time: ~15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear(input_height=64, input_width=36, n_neurons=5335, gamma=0.000, init_std=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnIjtCeERCjk",
    "outputId": "569f4934-9cb5-4253-ae02-001b6a9d7828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "correlation -8.0261256e-05\n",
      "poisson_loss 9588915.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 75/75 [00:00<00:00, 83.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001|00/05] ---> 0.005938975140452385\n",
      "=======================================\n",
      "correlation 0.005938975\n",
      "poisson_loss 3507831.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 75/75 [00:00<00:00, 84.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[002|00/05] ---> 0.010033603757619858\n",
      "=======================================\n",
      "correlation 0.010033604\n",
      "poisson_loss 3307697.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 75/75 [00:00<00:00, 76.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[003|00/05] ---> 0.014280364848673344\n",
      "=======================================\n",
      "correlation 0.014280365\n",
      "poisson_loss 3169816.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 75/75 [00:00<00:00, 81.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[004|00/05] ---> 0.01792152039706707\n",
      "=======================================\n",
      "correlation 0.01792152\n",
      "poisson_loss 3062201.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 75/75 [00:00<00:00, 82.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[005|00/05] ---> 0.021391743794083595\n",
      "=======================================\n",
      "correlation 0.021391744\n",
      "poisson_loss 2986927.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 75/75 [00:00<00:00, 83.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[006|00/05] ---> 0.02431696094572544\n",
      "=======================================\n",
      "correlation 0.024316961\n",
      "poisson_loss 2923228.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 75/75 [00:00<00:00, 79.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[007|00/05] ---> 0.02730812504887581\n",
      "=======================================\n",
      "correlation 0.027308125\n",
      "poisson_loss 2877410.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 75/75 [00:00<00:00, 82.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[008|00/05] ---> 0.02922097034752369\n",
      "=======================================\n",
      "correlation 0.02922097\n",
      "poisson_loss 2835198.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:  83%|████████▎ | 62/75 [00:00<00:00, 84.83it/s]"
     ]
    }
   ],
   "source": [
    "score, output, model_state = train_model(model=model, dataloader=dataloaders, seed=1, lr_init=1e-6, **trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "Y8Ouwn1YO_y_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.1460], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bn.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.2764], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m      \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           BatchNorm2d\n",
       "\u001b[0;31mString form:\u001b[0m    BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.8/dist-packages/torch/nn/modules/batchnorm.py\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BatchNorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\u001b[0m\n",
       "\u001b[0;34m    with additional channel dimension) as described in the paper\u001b[0m\n",
       "\u001b[0;34m    `Batch Normalization: Accelerating Deep Network Training by Reducing\u001b[0m\n",
       "\u001b[0;34m    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. math::\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    The mean and standard-deviation are calculated per-dimension over\u001b[0m\n",
       "\u001b[0;34m    the mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\u001b[0m\n",
       "\u001b[0;34m    of size `C` (where `C` is the input size). By default, the elements of :math:`\\gamma` are set\u001b[0m\n",
       "\u001b[0;34m    to 1 and the elements of :math:`\\beta` are set to 0. The standard-deviation is calculated\u001b[0m\n",
       "\u001b[0;34m    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Also by default, during training this layer keeps running estimates of its\u001b[0m\n",
       "\u001b[0;34m    computed mean and variance, which are then used for normalization during\u001b[0m\n",
       "\u001b[0;34m    evaluation. The running estimates are kept with a default :attr:`momentum`\u001b[0m\n",
       "\u001b[0;34m    of 0.1.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    If :attr:`track_running_stats` is set to ``False``, this layer then does not\u001b[0m\n",
       "\u001b[0;34m    keep running estimates, and batch statistics are instead used during\u001b[0m\n",
       "\u001b[0;34m    evaluation time as well.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. note::\u001b[0m\n",
       "\u001b[0;34m        This :attr:`momentum` argument is different from one used in optimizer\u001b[0m\n",
       "\u001b[0;34m        classes and the conventional notion of momentum. Mathematically, the\u001b[0m\n",
       "\u001b[0;34m        update rule for running statistics here is\u001b[0m\n",
       "\u001b[0;34m        :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\u001b[0m\n",
       "\u001b[0;34m        where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\u001b[0m\n",
       "\u001b[0;34m        new observed value.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Because the Batch Normalization is done over the `C` dimension, computing statistics\u001b[0m\n",
       "\u001b[0;34m    on `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        num_features: :math:`C` from an expected input of size\u001b[0m\n",
       "\u001b[0;34m            :math:`(N, C, H, W)`\u001b[0m\n",
       "\u001b[0;34m        eps: a value added to the denominator for numerical stability.\u001b[0m\n",
       "\u001b[0;34m            Default: 1e-5\u001b[0m\n",
       "\u001b[0;34m        momentum: the value used for the running_mean and running_var\u001b[0m\n",
       "\u001b[0;34m            computation. Can be set to ``None`` for cumulative moving average\u001b[0m\n",
       "\u001b[0;34m            (i.e. simple average). Default: 0.1\u001b[0m\n",
       "\u001b[0;34m        affine: a boolean value that when set to ``True``, this module has\u001b[0m\n",
       "\u001b[0;34m            learnable affine parameters. Default: ``True``\u001b[0m\n",
       "\u001b[0;34m        track_running_stats: a boolean value that when set to ``True``, this\u001b[0m\n",
       "\u001b[0;34m            module tracks the running mean and variance, and when set to ``False``,\u001b[0m\n",
       "\u001b[0;34m            this module does not track such statistics, and initializes statistics\u001b[0m\n",
       "\u001b[0;34m            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\u001b[0m\n",
       "\u001b[0;34m            When these buffers are ``None``, this module always uses batch statistics.\u001b[0m\n",
       "\u001b[0;34m            in both training and eval modes. Default: ``True``\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Shape:\u001b[0m\n",
       "\u001b[0;34m        - Input: :math:`(N, C, H, W)`\u001b[0m\n",
       "\u001b[0;34m        - Output: :math:`(N, C, H, W)` (same shape as input)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Examples::\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        >>> # With Learnable Parameters\u001b[0m\n",
       "\u001b[0;34m        >>> m = nn.BatchNorm2d(100)\u001b[0m\n",
       "\u001b[0;34m        >>> # Without Learnable Parameters\u001b[0m\n",
       "\u001b[0;34m        >>> m = nn.BatchNorm2d(100, affine=False)\u001b[0m\n",
       "\u001b[0;34m        >>> input = torch.randn(20, 100, 35, 45)\u001b[0m\n",
       "\u001b[0;34m        >>> output = m(input)\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected 4D input (got {}D input)'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                             \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.bn??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IVIV_2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
